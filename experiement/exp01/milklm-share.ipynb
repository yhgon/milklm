{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "398a96ee-b982-4eba-b4d1-068b87c2b22e",
   "metadata": {},
   "source": [
    "# milklm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c087b27b-4285-4597-b4c2-900cd17fd08a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e619aeb2-c59c-4bca-aafa-2b5db8ecf796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dbc270-620a-462d-96b5-35014e31453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file custom_data.py\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from tiktoken import encoding_for_model  # Assuming this is your tokenizer\n",
    "\n",
    "class StreamingParquetDataset(IterableDataset):\n",
    "    def __init__(self, dataset, config ):\n",
    "        self.dataset        =  dataset\n",
    "        self.block_size     = config.block_size\n",
    "        self.batch_size     = config.batch_size\n",
    "        self.tokenizer_name = config.tokenizer_name\n",
    "        self.buffer_size    = config.buffer_size        \n",
    "        self.buffer_size    = self.buffer_size * self.batch_size * (self.block_size + 1)  # Total tokens in buffer\n",
    "        self.tokenizer      = encoding_for_model(self.tokenizer_name)\n",
    "        self.buffer = []\n",
    "        self.iterator = iter(self.dataset)\n",
    "        self.load_next_buffer()\n",
    "\n",
    "    def load_next_buffer(self):\n",
    "        self.buffer = []\n",
    "        while len(self.buffer) < self.buffer_size:\n",
    "            try:\n",
    "                item = next(self.iterator)\n",
    "                text = item['text']\n",
    "                tokens = self.tokenizer.encode(text)\n",
    "                self.buffer.extend(tokens)\n",
    "            except StopIteration:\n",
    "                if not self.buffer:\n",
    "                    raise\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.buffer = []\n",
    "        self.iterator = iter(self.dataset)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if len(self.buffer) < self.batch_size * (self.block_size + 1):\n",
    "            self.load_next_buffer()\n",
    "\n",
    "        start = 0\n",
    "        end = self.batch_size * (self.block_size + 1)\n",
    "        chunk_tokens = self.buffer[start:end]\n",
    "        self.buffer = self.buffer[end:]\n",
    "\n",
    "        if len(chunk_tokens) < self.batch_size * (self.block_size + 1):\n",
    "            raise StopIteration\n",
    "\n",
    "        x = torch.tensor(chunk_tokens[:self.batch_size * self.block_size], dtype=torch.long).view(self.batch_size, self.block_size)\n",
    "        y = torch.tensor(chunk_tokens[1:self.batch_size * self.block_size + 1], dtype=torch.long).view(self.batch_size, self.block_size)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self,  dataset , config):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = config.batch_size\n",
    "        self.block_size = config.block_size\n",
    "        self.iterator = iter(self.dataset)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.iterator = iter(self.dataset)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self.iterator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7df11ab-ed66-466a-9383-e6943db10b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from modules import Block\n",
    "from config import ModelConfig\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config: ModelConfig, gradient_checkpointing=False):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.gradient_checkpointing = gradient_checkpointing\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            'wte': nn.Embedding(config.vocab_size, config.embedding_size),\n",
    "            'h': nn.ModuleList([Block(config, i, gradient_checkpointing) for i in range(config.num_layers)]),\n",
    "            'ln_f': nn.LayerNorm(config.embedding_size),\n",
    "        })\n",
    "        self.lm_head = nn.Linear(config.embedding_size, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.num_layers) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.max_context_length, f\"Cannot forward sequence of length {T}, block size is only {self.config.max_context_length}\"\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        x = tok_emb\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270286d3-b4a7-457f-a6ca-833f6f416883",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file utils.py\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tiktoken import get_encoding\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def load_tokens(filename, tokenizer_name='gpt-4o'):\n",
    "    \"\"\"Load and tokenize data from a file using tiktoken.\"\"\"\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "\n",
    "    # Use tiktoken's get_encoding to obtain an encoder for GPT2\n",
    "    encoder = get_encoding(tokenizer_name)\n",
    "    tokens = encoder.encode(data)\n",
    "    tokens = np.array(tokens, dtype=np.int32)\n",
    "\n",
    "    return torch.tensor(tokens, dtype=torch.long)\n",
    " \n",
    "\n",
    "def save_checkpoint(model, optimizer, model_config, train_config, data_config, epoch, loss):\n",
    "    # Extract necessary details from train_config\n",
    "    formatted_exp_num = f\"{train_config.exp_num:03d}\"\n",
    "    exp_name = train_config.exp_name\n",
    "    \n",
    "    # Create directory structure using exp_name and formatted exp_num\n",
    "    checkpoint_dir = os.path.join(exp_name, formatted_exp_num, 'checkpoints')\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # Save the model state\n",
    "    model_path = os.path.join(checkpoint_dir, f'model_state_{epoch:04d}.pth')\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': {'model_config': model_config.__dict__, 'data_config': data_config.__dict__, 'train_config': train_config.__dict__},\n",
    "        'epoch': epoch,\n",
    "        'loss': loss\n",
    "    }, model_path)\n",
    "\n",
    "    # Save the optimizer state\n",
    "    optimizer_path = os.path.join(checkpoint_dir, f'optimizer_state_{epoch:04d}.pth')\n",
    "    torch.save({\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, optimizer_path)\n",
    "\n",
    "\n",
    "def load_checkpoint(exp_name, exp_num, epoch, model, optimizer):\n",
    "    formatted_exp_num = f\"{exp_num:03d}\"\n",
    "    checkpoint_dir = os.path.join(exp_name, formatted_exp_num, 'checkpoints')\n",
    "    model_path = os.path.join(checkpoint_dir, f'model_state_{epoch:04d}.pth')\n",
    "    optimizer_path = os.path.join(checkpoint_dir, f'optimizer_state_{epoch:04d}.pth')\n",
    "\n",
    "    # Load model state\n",
    "    model_checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(model_checkpoint['model_state_dict'])\n",
    "\n",
    "    # Load optimizer state\n",
    "    optimizer_checkpoint = torch.load(optimizer_path)\n",
    "    optimizer.load_state_dict(optimizer_checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    return model_checkpoint.get('epoch'), model_checkpoint.get('loss'), model_checkpoint.get('config')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6018ee-7c86-4f68-bd77-3498d4d4b4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file eval.py\n",
    "import torch\n",
    "import argparse\n",
    "from model import Transformer\n",
    "from config import ModelConfig\n",
    "from utils import load_checkpoint\n",
    "from custom_data import DataLoaderLite\n",
    "\n",
    "def evaluate():\n",
    "    parser = argparse.ArgumentParser(description=\"Evaluate a GPT-like model.\")\n",
    "    parser.add_argument(\"--config\", type=str, required=True, help=\"Path to config file.\")\n",
    "    parser.add_argument(\"--checkpoint\", type=str, required=True, help=\"Path to model checkpoint.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    config = torch.load(args.config)\n",
    "    model_config = ModelConfig(**config['model_config'])\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Transformer(model_config).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "    load_checkpoint(args.checkpoint, model, optimizer)\n",
    "\n",
    "    val_loader = DataLoaderLite(config['data_config']['batch_size'], config['data_config']['block_size'], config['data_config']['dataset_dir'], \"val\")\n",
    "    model.eval()\n",
    "    val_loader.reset()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for _ in range(len(val_loader.tokens) // (config['data_config']['batch_size'] * config['data_config']['block_size'])):\n",
    "            x, y = val_loader.next_batch()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, loss = model(x, y)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Validation Loss: {total_loss / (len(val_loader.tokens) // (config['data_config']['batch_size'] * config['data_config']['block_size']))}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafb7119-f685-4909-9915-a2d65d5d72ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file train.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from model import Transformer\n",
    "from config import ModelConfig, DataConfig, TrainConfig\n",
    "from custom_data import StreamingParquetDataset, DataLoaderLite\n",
    "from utils import count_parameters, save_checkpoint, load_checkpoint, load_tokens\n",
    "from tiktoken import encoding_for_model\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from datasets import load_dataset\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def train_iter(model, dataloader, optimizer, scheduler, criterion, device, total_iters, max_total_iters, model_config, train_config, data_config, batch_size, block_size, tokenizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    iter_batch = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        if total_iters >= max_total_iters:\n",
    "            break\n",
    "\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        start_forward = time.time()\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            logits, loss = model(x, y)\n",
    "        forward_duration = (time.time() - start_forward) * 1000\n",
    "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), train_config.value_clip_grad_norm ) # default 1. use 0.5 \n",
    "        \n",
    "        start_backward = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "        scheduler.step()   # Update learning rate   \n",
    "        \n",
    "        backward_duration = (time.time() - start_backward) * 1000\n",
    "        iter_duration = (time.time() - start_forward)\n",
    "       \n",
    "        \n",
    "    # Print detailed log for each iteration        \n",
    "        if total_iters % train_config.print_interval_iteration ==0 : \n",
    "\n",
    "            perplexity = torch.exp(loss).item()\n",
    "            loss_val = loss.item()\n",
    "            norm_val = norm.item()\n",
    "            total_loss += loss_val\n",
    "    \n",
    "            tokens_total_seen = total_iters * (batch_size * block_size)\n",
    "            tokens_iter = (batch_size * block_size)\n",
    "            speed = (tokens_iter / iter_duration) / 1000\n",
    "    \n",
    "            # Decode input and generated text with line breaks replaced by \\n\n",
    "            input_text = tokenizer.decode(x[0].tolist()).replace('\\n', '\\\\n')\n",
    "            gt_text = tokenizer.decode(y[0].tolist()).replace('\\n', '\\\\n')\n",
    "                    # Check for valid token IDs before decoding\n",
    "            try:\n",
    "                pred_text = tokenizer.decode(torch.argmax(logits[0], dim=-1).tolist()).replace('\\n', '\\\\n')\n",
    "            except pyo3_runtime.PanicException as e:\n",
    "                pred_text = \"include <INVALID TOKEN ID>\"                              \n",
    "            \n",
    "            print_text_len = min(data_config.block_size,  train_config.print_token_len) \n",
    "            str_iter = f'\\nIter {total_iters:>6d}/{max_total_iters}, B{iter_batch + 1:>3d}, {tokens_total_seen / 1000000:3.1f} Mt'\n",
    "            str_loss = f'Loss {loss_val:>9.6f}, PPL {perplexity:>9.2f} {norm_val:>9.6f} | F {forward_duration:>5.1f} ms, B {backward_duration:>5.1f} ms | {speed:>4.2f} Kt/s'\n",
    "            str_text = f'O: {gt_text[-print_text_len:]} | P: {pred_text[-print_text_len:]} '            \n",
    "            print(f\"{str_iter} | {str_loss} | {str_text} ||\", end='')\n",
    "        total_iters += 1\n",
    "        iter_batch += 1\n",
    "\n",
    "        # Save checkpoint every 1000 iterations\n",
    "        if total_iters % train_config.save_interval_iter == 0:\n",
    "            save_checkpoint(model, optimizer, model_config, train_config, data_config, total_iters, total_loss / (batch_idx + 1))\n",
    "\n",
    "    return total_loss / (batch_idx + 1), total_iters\n",
    "\n",
    "def train(model, train_loader, optimizer, scheduler, criterion, device, model_config, train_config, data_config):\n",
    "    total_iters = 0\n",
    "    tokenizer = encoding_for_model(data_config.tokenizer_name)\n",
    "    max_total_iters = train_config.max_total_iters\n",
    "    \n",
    " \n",
    "\n",
    "    while total_iters < max_total_iters:\n",
    "        train_loss, total_iters = train_iter(model,  train_loader, optimizer, scheduler, criterion, device, total_iters, max_total_iters, model_config, train_config, data_config,data_config.batch_size, data_config.block_size, tokenizer)\n",
    " \n",
    "        \n",
    "        print(f'Total Iterations: {total_iters}/{max_total_iters} | Loss: {train_loss:>8.5f}')\n",
    "\n",
    "def get_lr_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, last_epoch=-1):\n",
    "    def lr_lambda(current_step: int):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "    \n",
    "\n",
    "def main():\n",
    "    print(\"start\")\n",
    "    parser = argparse.ArgumentParser(description=\"Train a GPT-like model.\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, help=\"Batch size for training.\")\n",
    "    parser.add_argument(\"--block_size\", type=int, help=\"Block size for training.\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, help=\"Learning rate for training.\")\n",
    "    parser.add_argument(\"--max_total_iters\", type=int, help=\"Maximum number of iterations for training.\")\n",
    "    parser.add_argument(\"--gradient_checkpointing\", action='store_true', help=\"Enable gradient checkpointing.\")\n",
    "    args = parser.parse_args()\n",
    "    tic = time.time()\n",
    "    # Load configurations from config.py\n",
    "    model_config = ModelConfig()\n",
    "    data_config = DataConfig()\n",
    "    train_config = TrainConfig()\n",
    "\n",
    "    # Update configurations with command-line arguments\n",
    "    if args.batch_size:\n",
    "        data_config.batch_size = args.batch_size\n",
    "    if args.block_size:\n",
    "        data_config.block_size = args.block_size\n",
    "    if args.learning_rate:\n",
    "        train_config.learning_rate = args.learning_rate\n",
    "    if args.max_total_iters:\n",
    "        train_config.max_total_iters = args.max_total_iters\n",
    "    if args.gradient_checkpointing:\n",
    "        train_config.gradient_checkpointing = args.gradient_checkpointing\n",
    "\n",
    "    print(model_config)\n",
    "    print(data_config)\n",
    "    print(train_config)\n",
    "\n",
    "    # Set the float32 matmul precision to 'medium'\n",
    "    torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "    # Setup device and model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.manual_seed(1337)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1337)\n",
    "\n",
    "    model = Transformer(model_config, train_config.gradient_checkpointing).to(device)\n",
    "    toc = time.time()\n",
    "    dur = toc - tic\n",
    "    print(f\"model configure {dur:4.1f}sec\")\n",
    "\n",
    "    tic = time.time()\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=train_config.learning_rate, betas=(0.9, 0.95), eps=1e-8)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Define the learning rate scheduler\n",
    "    num_training_steps = train_config.max_total_iters\n",
    "    scheduler = get_lr_schedule_with_warmup(optimizer, num_warmup_steps=train_config.num_warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "\n",
    "    toc = time.time()\n",
    "    dur = toc - tic\n",
    "    print(f\"configure optimizer during {dur:4.1f}sec\")\n",
    "    print(f\"{count_parameters(model) / 1000000} M Params\")\n",
    "    print(model)\n",
    "\n",
    "    # Setup data loaders\n",
    "    print(\"prepare dataset\")\n",
    "    tic = time.time()\n",
    "    # Load and prepare data\n",
    "    try:\n",
    "        dataset = load_dataset('parquet', data_files=f'{data_config.dataset_dir}/**/*.parquet', split='train', streaming=True)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "    train_dataset = StreamingParquetDataset(dataset, data_config )\n",
    "    \n",
    "    toc = time.time()\n",
    "    dur = toc - tic\n",
    "    print(f\"load Parquet Data {dur:4.2f}sec\")\n",
    "\n",
    "    tic = time.time()\n",
    "    train_loader = DataLoaderLite(train_dataset,  data_config )\n",
    "    toc = time.time()\n",
    "    dur_data = toc - tic\n",
    "    print(f\"configure DataLoaderLite during {dur_data:4.1f} sec\")\n",
    "\n",
    "    # Train the model\n",
    "    print(\"start train\")\n",
    "    train(model, train_loader, optimizer, scheduler, criterion, device, model_config, train_config, data_config)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81066b9-6116-484a-9679-6e8dea80d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file modules.py \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config, layer_idx):\n",
    "        super().__init__()\n",
    "        assert config.embedding_size % config.num_heads == 0\n",
    "        self.c_attn = nn.Linear(config.embedding_size, 3 * config.embedding_size)\n",
    "        self.c_proj = nn.Linear(config.embedding_size, config.embedding_size)\n",
    "        self.n_head = config.num_heads\n",
    "        self.n_embd = config.embedding_size\n",
    "        self.layer_idx = layer_idx\n",
    "        self.debug = config.debug\n",
    "        self.proj_ratio = config.proj_ratio\n",
    "        self.proj_ratio_min = config.proj_ratio_min\n",
    "        self.num_keep_boundary_chunk = config.num_keep_boundary_chunk\n",
    "\n",
    "        # Initialize cache for projection matrices\n",
    "        self.proj_matrix_cache = {}\n",
    "\n",
    "    def _apply_rotary_embedding(self, x, seq_len):\n",
    "        dim = x.shape[-1]\n",
    "        dtype = x.dtype\n",
    "\n",
    "        theta = 10000.0\n",
    "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        freqs = torch.outer(torch.arange(seq_len), freqs).to(x.device)\n",
    "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "\n",
    "        if dtype not in [torch.float32, torch.float64]:\n",
    "            x_temp = x.float()\n",
    "        else:\n",
    "            x_temp = x\n",
    "\n",
    "        x_temp = x_temp.view(*x_temp.shape[:-1], x_temp.shape[-1] // 2, 2)\n",
    "        x_temp = torch.view_as_complex(x_temp)\n",
    "\n",
    "        x_temp = x_temp * freqs_cis\n",
    "        x_rot = torch.view_as_real(x_temp).view(*x.shape[:-1], -1)\n",
    "        if x.dtype != x_rot.dtype:\n",
    "            x_rot = x_rot.to(dtype)\n",
    "\n",
    "        return x_rot\n",
    "\n",
    "    def _generate_projection_matrix(self, T, P, exponent=2):\n",
    "        key = (T, P, exponent)\n",
    "        if key in self.proj_matrix_cache:\n",
    "            return self.proj_matrix_cache[key]\n",
    "\n",
    "        start = 0\n",
    "        end = 2595 * np.log10(1 + (T / 2) / 700.0)\n",
    "        steps = np.linspace(start, end, P + 2)\n",
    "        scales = 700 * (10**(steps / 2595) - 1)\n",
    "\n",
    "        bins = np.floor((T + 1) * scales / (T / 2)).astype(int)\n",
    "        bins = np.clip(bins, 0, T)  # Ensure bins are within the valid range\n",
    "        basis_matrix = np.zeros((P, T))\n",
    "\n",
    "        for p in range(1, P + 1):\n",
    "            start_bin = bins[p - 1]\n",
    "            mid_bin = bins[p]\n",
    "            end_bin = bins[min(p + 1, len(bins) - 1)]\n",
    "\n",
    "            for t in range(start_bin, mid_bin):\n",
    "                if bins[p] - bins[p - 1] != 0:\n",
    "                    basis_matrix[p - 1, t] = ((t - bins[p - 1]) / (bins[p] - bins[p - 1])) ** exponent\n",
    "            for t in range(mid_bin, end_bin):\n",
    "                if bins[min(p + 1, len(bins) - 1)] - bins[p] != 0:\n",
    "                    basis_matrix[p - 1, t] = ((bins[min(p + 1, len(bins) - 1)] - t) / (bins[min(p + 1, len(bins) - 1)] - bins[p])) ** exponent\n",
    "\n",
    "        self.proj_matrix_cache[key] = basis_matrix\n",
    "        return basis_matrix\n",
    "\n",
    "    def _get_boundary_tokens(self, T):\n",
    "        chunk_size = T // self.proj_ratio\n",
    "        num_keep = self.num_keep_boundary_chunk\n",
    "        tokens_front_boundary_chunk = torch.arange(0, num_keep * chunk_size)\n",
    "        tokens_last_boundary_chunk = torch.arange(T - num_keep * chunk_size, T)\n",
    "        return tokens_front_boundary_chunk, tokens_last_boundary_chunk\n",
    "\n",
    "    def _get_body_tokens(self, T):\n",
    "        chunk_size = T // self.proj_ratio\n",
    "        num_keep = self.num_keep_boundary_chunk\n",
    "        tokens_body_chunk = torch.arange(num_keep * chunk_size, T - num_keep * chunk_size)\n",
    "        return tokens_body_chunk\n",
    "\n",
    "    def _project_tokens(self, x, tokens):\n",
    "        P = tokens.numel() // self.proj_ratio\n",
    "        projection_matrix = self._generate_projection_matrix(tokens.numel(), P, exponent=3)\n",
    "        projection_matrix = torch.tensor(projection_matrix, dtype=torch.float32, device=x.device).T\n",
    "        x_se = torch.einsum('bhtd,tp->bhpd', x[:, :, tokens], projection_matrix)\n",
    "        return x_se\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        if self.debug:\n",
    "            print(f\"{self.layer_idx}L - mhsa - shape of x {x.shape} input \")\n",
    "\n",
    "        qkv = self.c_attn(x)\n",
    "        if self.debug:\n",
    "            print(f\"{self.layer_idx}L - mhsa - shape of x {x.shape} after qkv projection\")\n",
    "\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        if self.debug:\n",
    "            print(f\"{self.layer_idx}L - mhsa - shape of Q {q.shape} after split \")\n",
    "            print(f\"{self.layer_idx}L - mhsa - shape of K {k.shape} after split\")\n",
    "            print(f\"{self.layer_idx}L - mhsa - shape of V {v.shape} after split\")\n",
    "\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"{self.layer_idx}L - mhsa - shape of Q {q.shape} after view.transpose(1,2) \")\n",
    "            print(f\"{self.layer_idx}L - mhsa - shape of K {k.shape} after view.transpose(1,2)\")\n",
    "            print(f\"{self.layer_idx}L - mhsa - shape of V {v.shape} after view.transpose(1,2)\")\n",
    "\n",
    "        q = self._apply_rotary_embedding(q, T)\n",
    "        k = self._apply_rotary_embedding(k, T)\n",
    "        # v = self._apply_rotary_embedding(v, T)\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"{self.layer_idx}L - mhsa - shape of Q {q.shape} after RoPE \")\n",
    "            print(f\"{self.layer_idx}L - mhsa - shape of K {k.shape} after RoPE\")\n",
    "            print(f\"{self.layer_idx}L - mhsa - shape of V {v.shape} after RoPE\")\n",
    "\n",
    "        tokens_front_boundary_chunk, tokens_last_boundary_chunk = self._get_boundary_tokens(T)\n",
    "        tokens_body_chunk = self._get_body_tokens(T)\n",
    "\n",
    "        k_se_front_boundary = k[:, :, tokens_front_boundary_chunk]\n",
    "        v_se_front_boundary = v[:, :, tokens_front_boundary_chunk]\n",
    "\n",
    "        k_se_body = self._project_tokens(k, tokens_body_chunk)\n",
    "        v_se_body = self._project_tokens(v, tokens_body_chunk)\n",
    "\n",
    "        k_se_last_boundary = k[:, :, tokens_last_boundary_chunk]\n",
    "        v_se_last_boundary = v[:, :, tokens_last_boundary_chunk]\n",
    "\n",
    "        k_se = torch.cat([k_se_front_boundary, k_se_body, k_se_last_boundary], dim=-2)\n",
    "        v_se = torch.cat([v_se_front_boundary, v_se_body, v_se_last_boundary], dim=-2)\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"{self.layer_idx}L - mhsa - shape of k_se {k_se.shape} after projection\")\n",
    "            print(f\"{self.layer_idx}L - mhsa - shape of v_se {v_se.shape} after projection\")\n",
    "\n",
    "        att = (q @ k_se.transpose(-2, -1)) * (1.0 / math.sqrt(k_se.size(-1)))\n",
    "        if self.debug:\n",
    "            print(f\"{self.layer_idx}L - mhsa - shape of att score matrix {att.shape} after matmul\")\n",
    "\n",
    "        mask = torch.tril(torch.ones((T, k_se.size(-2)), device=x.device, dtype=torch.bool))\n",
    "        if self.debug:\n",
    "            print(f\"{self.layer_idx}L - mhsa - shape of mask {mask.shape}\")\n",
    "\n",
    "        mask = mask[None, None, :, :]  # Add dimensions for batch and head\n",
    "        if self.debug:\n",
    "            print(f\"{self.layer_idx}L - mhsa - shape of mask {mask.shape} after reshape\")\n",
    "\n",
    "        att = att.masked_fill(~mask[:, None, None, :], float('-inf'))\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"{self.layer_idx}L - mhsa - shape of att score matrix {att.shape} after mask fill\")\n",
    "\n",
    "        #att = F.sigmoid(att)\n",
    "        att =  F.softmax(att, dim=-1)\n",
    "        if self.debug:\n",
    "            print(f\"{self.layer_idx}L - mhsa - shape of att score matrix {att.shape} after sigmoid\")\n",
    "\n",
    "        x = att @ v_se  # (B,nh,T,P) x (B,nh,P,hs) --> ( B, nh, T, hs)\n",
    "        if self.debug:\n",
    "            print(f\"{self.layer_idx}L - mhsa - shape of x {x.shape} after matmul\")\n",
    "\n",
    "        x = x.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        if self.debug:\n",
    "            print(f\"{self.layer_idx}L - mhsa - shape of x {x.shape} after view(B,T,C)\")\n",
    "\n",
    "        x = self.c_proj(x)\n",
    "        if self.debug:\n",
    "            print(f\"{self.layer_idx}L - mhsa - shape of x {x.shape} after out projection\")\n",
    "        return x\n",
    "\n",
    "# Rest of the code remains unchanged\n",
    "\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config, layer_idx):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.embedding_size, int(config.embedding_size * config.ffn_scale_ratio))\n",
    "        self.gelu = nn.GELU()\n",
    "        self.silu = F.silu\n",
    "        self.c_proj = nn.Linear(int(config.embedding_size * config.ffn_scale_ratio), config.embedding_size)\n",
    "        self.layer_idx = layer_idx\n",
    "        self.debug = config.debug\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_fc = self.c_fc(x)\n",
    "        if self.debug:\n",
    "            print(f\"{self.layer_idx}L - mlp - shape of x_fc {x_fc.shape}\")\n",
    "        x_gelu = self.silu(x_fc)\n",
    "        x_proj = self.c_proj(x_gelu)\n",
    "        if self.debug:\n",
    "            print(f\"{self.layer_idx}L - mlp - shape of x_proj {x_proj.shape}\")\n",
    "        return x_proj\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config, layer_idx, gradient_checkpointing=False):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.embedding_size)\n",
    "        self.attn = CausalSelfAttention(config, layer_idx)\n",
    "        self.ln_2 = nn.LayerNorm(config.embedding_size)\n",
    "        self.mlp = MLP(config, layer_idx)\n",
    "        self.gradient_checkpointing = gradient_checkpointing\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.gradient_checkpointing and self.training:\n",
    "            x = x + checkpoint(self.attn, self.ln_1(x))\n",
    "            x = x + checkpoint(self.mlp, self.ln_2(x))\n",
    "        else:\n",
    "            x = x + self.attn(self.ln_1(x))\n",
    "            x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b2ab31-006d-47ab-85e8-c1f006fd3314",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file config.py\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    embedding_size: int = 1024\n",
    "    num_layers: int = 32\n",
    "    num_heads: int = 16\n",
    "    ffn_scale_ratio: float = 0.5\n",
    "    max_context_length: int = 8192\n",
    "    vocab_size: int = 200019  #  200960 for 200019 for gpt-4o |   251264 for  250257 for gpt2 with 1000 token buffers\n",
    "    debug: bool = False  # Add debug flag\n",
    "    proj_ratio_min : int = 2\n",
    "    proj_ratio : int = 10\n",
    "    num_keep_boundary_chunk : int = 2\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    batch_size: int =60\n",
    "    block_size: int = 64\n",
    "    buffer_size : int = 10 \n",
    "    tokenizer_name: str = \"gpt-4o\"\n",
    "    dataset_dir: str = \"/mnt/e/jupyter/gpt2_scratch2/datasets/finewebedu/CC-MAIN-2024-10\"\n",
    "    truncate_limit: int = 1000000000\n",
    "    shuffle_tokens: bool = False   \n",
    "    mask_random: bool = False   \n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    learning_rate: float = 1e-3\n",
    "    dtype: str = \"bfloat16\"\n",
    "    optimizer_options: dict = None\n",
    "    exp_name: str = \"experiment\"\n",
    "    exp_num: int = 8\n",
    "    save_interval_epoch: int = 10\n",
    "    save_interval_iter: int = 2000\n",
    "    print_interval_iteration: int = 10\n",
    "    eval_interval_iteration: int = 250\n",
    "    num_epochs: int = 200\n",
    "    max_total_iters : int = 3000000\n",
    "    num_warmup_steps : int = 4000\n",
    "    gradient_checkpointing : bool = True\n",
    "    value_clip_grad_norm : float = 0.5\n",
    "    lr_step : int = 1000\n",
    "    print_token_len : int = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de816ae-c0f3-4f75-8dee-7db9fbef7800",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62a6240-2c3a-4b3a-89fa-f7f3c82c9a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabb5748-73a1-4ded-bdaf-72e0f86f41df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
