{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4144c765-03e8-4f5a-b0ad-e453a7be2c9f",
   "metadata": {},
   "source": [
    "# evaluate autoregressive infeerence\n",
    "## this code assume no kvcache mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c212a2-023f-4602-b855-42a853637bee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c02d016-b6b9-442f-8658-c71fb9e15713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing evaluate_new.py\n"
     ]
    }
   ],
   "source": [
    "%%file evaluate_new.py\n",
    "import torch\n",
    "import argparse\n",
    "import time\n",
    "from model import Transformer\n",
    "from tiktoken import encoding_for_model\n",
    "from config import ModelConfig, DataConfig, TrainConfig\n",
    "\n",
    "def evaluate(model, tokenizer, input_text, device, max_length=100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Tokenize input text\n",
    "        input_ids = tokenizer.encode(input_text)\n",
    "        input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "\n",
    " \n",
    "        # Autoregressive generation\n",
    "        generated = input_tensor\n",
    "        start_time = time.time()\n",
    "        idx_tokens = 0        \n",
    "        \n",
    "        for _ in range(max_length):\n",
    "\n",
    "            logits, _ = model(generated, generated)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "            \n",
    "            # Stop if EOS token is generated\n",
    "            print(tokenizer.decode([next_token.item()]), end='', flush=True)\n",
    "            \n",
    "            if idx_tokens >64:\n",
    "                break\n",
    "            idx_tokens =idx_tokens+1\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Decode output tokens\n",
    "        output_text = tokenizer.decode(generated[0].tolist())\n",
    "\n",
    "        # Calculate generation speed\n",
    "        num_generated_tokens = generated.size(1) - input_tensor.size(1)\n",
    "        generation_time = end_time - start_time\n",
    "        tokens_per_second = num_generated_tokens / generation_time\n",
    "\n",
    "        return output_text, tokens_per_second\n",
    "\n",
    "def load_model_and_tokenizer(checkpoint_path, device):\n",
    "    print(\"start to load file\")\n",
    "    # Load model checkpoint\n",
    "    tic = time.time()\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    toc = time.time()\n",
    "    dur = toc-tic \n",
    "    print(f\" complete to load {checkpoint_path} during {dur:4.2f}sec \")\n",
    "    # Extract configurations from the checkpoint \n",
    "    config = checkpoint['config']\n",
    "    model_config = ModelConfig(**config['model_config'])\n",
    "    train_config = TrainConfig(**config['train_config'])\n",
    "    data_config = DataConfig(**config['data_config'])\n",
    "\n",
    "    print(model_config)\n",
    "    print(train_config)\n",
    "    print(data_config)\n",
    "    # Initialize model\n",
    "    print(\"config model\")\n",
    "    tic=time.time()\n",
    "    model = Transformer(model_config, train_config.gradient_checkpointing).to(device)\n",
    "    toc = time.time()\n",
    "    dur = toc - tic \n",
    "    print(f\"configure model complete with {dur:4.2f}sec\")\n",
    "    print(model)\n",
    "    print(\"load model state\")\n",
    "    tic = time.time()\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    toc = time.time()\n",
    "    dur = toc - tic \n",
    "    print(f\" load model state  {dur:4.2f}sec\")\n",
    "    # Initialize tokenizer\n",
    "    tokenizer_name = data_config.tokenizer_name\n",
    "    tokenizer = encoding_for_model(tokenizer_name)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Evaluate a GPT-like model from a checkpoint.\")\n",
    "    parser.add_argument(\"--model_checkpoint_path\", type=str, required=True, help=\"Path to the model checkpoint file.\")\n",
    "    parser.add_argument(\"--input_text\", type=str, required=True, help=\"Input text for evaluation.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Initialize device\n",
    "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device='cpu'\n",
    "    print(\"start\")\n",
    "    print(f\"Input Text: {args.input_text}\")\n",
    "    print(\"load model and tokenizer\")\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    tic_load = time.time()\n",
    "    model, tokenizer = load_model_and_tokenizer(args.model_checkpoint_path, device)\n",
    "    toc_lod = time.time()\n",
    "    dur_load = toc_load - tic_load \n",
    "    print(f\" model load time {dur_load:4.1f}sec\")\n",
    "\n",
    "    # Evaluate input text\n",
    "    print(f\"Input Text: {args.input_text}\")\n",
    "    \n",
    "    output_text, tokens_per_second = evaluate(model, tokenizer, args.input_text, device)\n",
    "    \n",
    "    #print(f\"Output Text: {output_text}\")\n",
    "    print(f\"Generation Speed: {tokens_per_second:.2f} tokens/sec\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b1d49f-c480-4b2e-bc97-ec8853201391",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate_new.py --model_checkpoint_path './experiment/006/checkpoints/model_state_16000.pth' --input_text \"The large langugae model is\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
